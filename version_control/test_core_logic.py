"""
ç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿæ ¸å¿ƒé€»è¾‘éªŒè¯

éªŒè¯æ ¸å¿ƒåŠŸèƒ½é€»è¾‘æ˜¯å¦æ­£ç¡®ï¼ˆä¸ä¾èµ–å¤–éƒ¨åº“ï¼‰
"""

import sys
import os
import json
from pathlib import Path
from datetime import datetime

def test_core_logic():
    """æµ‹è¯•æ ¸å¿ƒé€»è¾‘åŠŸèƒ½"""
    print("ğŸš€ å¼€å§‹æ ¸å¿ƒé€»è¾‘éªŒè¯...")
    
    try:
        # 1. æµ‹è¯•é…ç½®ç³»ç»Ÿ
        print("\n1. æµ‹è¯•é…ç½®ç³»ç»Ÿ...")
        from config import get_config, VersionControlConfig
        
        config = get_config("default")
        assert config.base_dir == "./version_control_workspace"
        assert config.auto_versioning == True
        print("âœ… é…ç½®ç³»ç»ŸéªŒè¯é€šè¿‡")
        
        # 2. æµ‹è¯•å®éªŒè·Ÿè¸ªå™¨çš„æ ¸å¿ƒé€»è¾‘
        print("\n2. æµ‹è¯•å®éªŒè·Ÿè¸ªå™¨...")
        from experiment_tracker import ExperimentTracker
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        test_dir = "./test_experiments_core"
        tracker = ExperimentTracker(test_dir)
        
        # æµ‹è¯•å®éªŒåˆ›å»º
        exp_id = tracker.start_experiment(
            "æ ¸å¿ƒé€»è¾‘æµ‹è¯•",
            "æµ‹è¯•æè¿°",
            tags=["test"]
        )
        assert exp_id is not None
        assert len(exp_id) > 0
        
        # æµ‹è¯•å‚æ•°è®°å½•
        test_params = {"lr": 0.001, "batch_size": 32}
        tracker.log_hyperparameters(exp_id, test_params)
        
        # æµ‹è¯•æŒ‡æ ‡è®°å½•
        test_metrics = {"loss": 0.5, "accuracy": 0.85}
        tracker.log_metrics(exp_id, test_metrics, epoch=1)
        
        # æµ‹è¯•å®éªŒç»“æŸ
        tracker.end_experiment(exp_id, "completed")
        
        # éªŒè¯å®éªŒæ•°æ®
        exp_data = tracker.get_experiment(exp_id)
        assert exp_data['name'] == "æ ¸å¿ƒé€»è¾‘æµ‹è¯•"
        assert exp_data['status'] == "completed"
        assert "lr" in exp_data['hyperparameters']
        
        print("âœ… å®éªŒè·Ÿè¸ªå™¨æ ¸å¿ƒé€»è¾‘éªŒè¯é€šè¿‡")
        
        # 3. æµ‹è¯•ç‰ˆæœ¬ç®¡ç†å™¨çš„æ ¸å¿ƒé€»è¾‘
        print("\n3. æµ‹è¯•ç‰ˆæœ¬ç®¡ç†å™¨...")
        from model_version_manager import ModelVersionManager
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        version_dir = "./test_versions_core"
        version_manager = ModelVersionManager(version_dir)
        
        # æ¨¡æ‹Ÿæ¨¡å‹ä¿¡æ¯ï¼ˆä¸ä½¿ç”¨çœŸå®çš„PyTorchæ¨¡å‹ï¼‰
        mock_model_info = {
            'class_name': 'MockModel',
            'model_type': 'MockModel',
            'parameters_count': 1000000,
            'trainable_parameters': 900000
        }
        
        # æ¨¡æ‹Ÿä¿å­˜ç‰ˆæœ¬ä¿¡æ¯
        version_info = {
            'version_name': 'test_model_v1',
            'created_at': datetime.now().isoformat(),
            'description': 'æµ‹è¯•æ¨¡å‹ç‰ˆæœ¬',
            'model_info': mock_model_info,
            'metadata': {'test': True},
        }
        
        # ç›´æ¥æµ‹è¯•ç‰ˆæœ¬ä¿¡æ¯ç®¡ç†
        version_manager.versions['test_model_v1'] = version_info
        version_manager._save_versions_index()
        
        # æµ‹è¯•ç‰ˆæœ¬åˆ—è¡¨
        versions = version_manager.list_versions()
        assert len(versions) > 0
        assert versions[0]['version_name'] == 'test_model_v1'
        
        print("âœ… ç‰ˆæœ¬ç®¡ç†å™¨æ ¸å¿ƒé€»è¾‘éªŒè¯é€šè¿‡")
        
        # 4. æµ‹è¯•æ€§èƒ½å¯¹æ¯”å™¨çš„æ•°æ®å¤„ç†é€»è¾‘
        print("\n4. æµ‹è¯•æ€§èƒ½å¯¹æ¯”å™¨...")
        from performance_comparator import PerformanceComparator
        
        comparator = PerformanceComparator("./test_results_core")
        
        # æ¨¡æ‹Ÿæ¨¡å‹è¯„ä¼°ç»“æœ
        mock_result1 = {
            'model_name': 'Model_A',
            'evaluation_time': datetime.now().isoformat(),
            'metrics': {
                'accuracy': 0.85,
                'precision': 0.83,
                'recall': 0.87,
                'f1_score': 0.85
            },
            'performance': {
                'avg_inference_time': 0.001,
                'samples_per_second': 1000
            },
            'model_complexity': {
                'total_parameters': 1000000,
                'model_size_mb': 10.0
            }
        }
        
        mock_result2 = {
            'model_name': 'Model_B',
            'evaluation_time': datetime.now().isoformat(),
            'metrics': {
                'accuracy': 0.87,
                'precision': 0.85,
                'recall': 0.89,
                'f1_score': 0.87
            },
            'performance': {
                'avg_inference_time': 0.002,
                'samples_per_second': 500
            },
            'model_complexity': {
                'total_parameters': 2000000,
                'model_size_mb': 20.0
            }
        }
        
        # æµ‹è¯•æ¯”è¾ƒåŠŸèƒ½
        comparison = comparator.compare_models([mock_result1, mock_result2], "test_comparison")
        
        assert 'best_models' in comparison
        assert 'summary_table' in comparison
        assert len(comparison['summary_table']) == 2
        
        # æµ‹è¯•A/Bæµ‹è¯•
        ab_result = comparator.ab_test(mock_result1, mock_result2)
        assert 'summary' in ab_result
        assert 'recommendation' in ab_result['summary']
        
        print("âœ… æ€§èƒ½å¯¹æ¯”å™¨æ ¸å¿ƒé€»è¾‘éªŒè¯é€šè¿‡")
        
        # 5. æ¸…ç†æµ‹è¯•æ–‡ä»¶
        print("\n5. æ¸…ç†æµ‹è¯•æ–‡ä»¶...")
        import shutil
        test_dirs = [test_dir, version_dir, "./test_results_core"]
        for test_dir_path in test_dirs:
            if Path(test_dir_path).exists():
                shutil.rmtree(test_dir_path)
        print("âœ… æµ‹è¯•æ–‡ä»¶æ¸…ç†å®Œæˆ")
        
        print("\nğŸ‰ æ‰€æœ‰æ ¸å¿ƒé€»è¾‘éªŒè¯é€šè¿‡ï¼")
        print("\nç³»ç»ŸçŠ¶æ€: æ ¸å¿ƒåŠŸèƒ½å°±ç»ª âœ…")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ æ ¸å¿ƒé€»è¾‘éªŒè¯å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("="*60)
    print("ç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿ - æ ¸å¿ƒé€»è¾‘éªŒè¯")
    print("="*60)
    
    success = test_core_logic()
    
    if success:
        print("\n" + "="*60)
        print("æ ¸å¿ƒé€»è¾‘éªŒè¯æˆåŠŸ - ç³»ç»Ÿå°±ç»ªï¼")
        print("\næ³¨æ„: å®Œæ•´åŠŸèƒ½éœ€è¦å®‰è£… PyTorch, matplotlib, seaborn ç­‰ä¾èµ–")
        print("å®‰è£…å‘½ä»¤: pip install torch matplotlib seaborn pandas scikit-learn")
        print("="*60)
    else:
        print("\n" + "="*60)
        print("æ ¸å¿ƒé€»è¾‘éªŒè¯å¤±è´¥")
        print("="*60)
        sys.exit(1)
