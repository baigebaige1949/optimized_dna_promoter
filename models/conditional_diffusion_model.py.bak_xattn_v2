"""
条件扩散模型 - 简化高效实现
支持多类型条件输入和交叉注意力机制
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, Any, Optional, Tuple, List
import math


class ConditionalEmbedding(nn.Module):
    """条件嵌入层：支持温度、pH、氧气等多种条件"""
    
    def __init__(self, condition_dim: int = 64, num_conditions: int = 8):
        super().__init__()
        self.condition_dim = condition_dim
        self.num_conditions = num_conditions
        
        # 各种条件的嵌入层
        self.temperature_embed = nn.Linear(1, condition_dim)
        self.ph_embed = nn.Linear(1, condition_dim)
        self.oxygen_embed = nn.Linear(1, condition_dim)
        self.salt_embed = nn.Linear(1, condition_dim)
        self.carbon_embed = nn.Linear(1, condition_dim)
        self.nitrogen_embed = nn.Linear(1, condition_dim)
        self.glucose_embed = nn.Linear(1, condition_dim)
        self.pressure_embed = nn.Linear(1, condition_dim)
        
        # 融合层
        self.fusion = nn.Sequential(
            nn.Linear(num_conditions * condition_dim, condition_dim * 2),
            nn.ReLU(),
            nn.Linear(condition_dim * 2, condition_dim),
            nn.LayerNorm(condition_dim)
        )
        
        # 智能默认值
        self.default_conditions = {
            'temperature': 37.0,  # 37°C
            'ph': 7.0,           # 中性pH
            'oxygen': 0.21,      # 21% 氧气
            'salt': 0.15,        # 0.15 M NaCl
            'carbon': 2.0,       # 2% glucose
            'nitrogen': 0.1,     # 0.1% nitrogen source
            'glucose': 2.0,      # 2% glucose
            'pressure': 1.0      # 1 atm
        }
    
    def fill_default_conditions(self, conditions: Optional[Dict[str, float]]) -> Dict[str, float]:
        """智能默认条件填充"""
        if conditions is None:
            return self.default_conditions.copy()
        
        filled_conditions = self.default_conditions.copy()
        filled_conditions.update(conditions)
        return filled_conditions
    
    def forward(self, conditions: Optional[Dict[str, float]] = None) -> torch.Tensor:
        """前向传播"""
        # 智能填充默认值
        conditions = self.fill_default_conditions(conditions)
        
        # 转换为tensor
        device = next(self.parameters()).device
        embeddings = []
        
        for key in ['temperature', 'ph', 'oxygen', 'salt', 'carbon', 'nitrogen', 'glucose', 'pressure']:
            value = torch.tensor([[conditions[key]]], device=device, dtype=torch.float32)
            embed_layer = getattr(self, f"{key}_embed")
            embeddings.append(embed_layer(value))
        
        # 拼接所有嵌入
        combined = torch.cat(embeddings, dim=-1)  # [1, num_conditions * condition_dim]
        
        # 融合处理
        return self.fusion(combined)  # [1, condition_dim]


class CrossAttention(nn.Module):
    """交叉注意力机制：融合条件信息"""
    
    def __init__(self, dim: int, condition_dim: int, num_heads: int = 8):
        super().__init__()
        self.dim = dim
        self.condition_dim = condition_dim
        self.num_heads = num_heads
        self.scale = (dim // num_heads) ** -0.5
        
        self.to_q = nn.Linear(dim, dim, bias=False)
        self.to_k = nn.Linear(condition_dim, dim, bias=False)
        self.to_v = nn.Linear(condition_dim, dim, bias=False)
        self.to_out = nn.Linear(dim, dim)
        
    def forward(self, x: torch.Tensor, condition: torch.Tensor) -> torch.Tensor:
        """
        x: [batch, seq_len, dim] - 输入特征
        condition: [batch, condition_dim] - 条件特征
        """
        batch, seq_len, _ = x.shape
        
        q = self.to_q(x)  # [batch, seq_len, dim]
        k = self.to_k(condition).unsqueeze(1)  # [batch, 1, dim]
        v = self.to_v(condition).unsqueeze(1)  # [batch, 1, dim]
        
        # Multi-head attention
        q = q.view(batch, seq_len, self.num_heads, -1).transpose(1, 2)
        k = k.view(batch, 1, self.num_heads, -1).transpose(1, 2)
        v = v.view(batch, 1, self.num_heads, -1).transpose(1, 2)
        
        attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale
        attn = F.softmax(attn, dim=-1)
        
        out = torch.matmul(attn, v)
        out = out.transpose(1, 2).contiguous().view(batch, seq_len, -1)
        
        return self.to_out(out)


class ConditionalUNetBlock(nn.Module):
    """条件U-Net基础块"""
    
    def __init__(self, in_dim: int, out_dim: int, condition_dim: int, time_dim: int = 128):
        super().__init__()
        self.in_dim = in_dim
        self.out_dim = out_dim
        
        # 时间嵌入
        self.time_mlp = nn.Sequential(
            nn.Linear(time_dim, out_dim),
            nn.ReLU(),
            nn.Linear(out_dim, out_dim)
        )
        
        # 主要卷积层
        self.conv1 = nn.Conv1d(in_dim, out_dim, 3, padding=1)
        self.conv2 = nn.Conv1d(out_dim, out_dim, 3, padding=1)
        
        # 条件交叉注意力
        self.cross_attn = CrossAttention(out_dim, condition_dim)
        
        # 归一化层
        self.norm1 = nn.GroupNorm(8, out_dim)
        self.norm2 = nn.GroupNorm(8, out_dim)
        
        # 残差连接
        self.residual = nn.Conv1d(in_dim, out_dim, 1) if in_dim != out_dim else nn.Identity()
        
    def forward(self, x: torch.Tensor, time_emb: torch.Tensor, condition: torch.Tensor) -> torch.Tensor:
        """
        x: [batch, in_dim, seq_len]
        time_emb: [batch, time_dim]
        condition: [batch, condition_dim]
        """
        residual = self.residual(x)
        
        # 第一个卷积
        h = self.conv1(x)
        h = self.norm1(h)
        h = F.relu(h)
        
        # 时间嵌入
        time_proj = self.time_mlp(time_emb)[:, :, None]  # [batch, out_dim, 1]
        h = h + time_proj
        
        # 第二个卷积
        h = self.conv2(h)
        h = self.norm2(h)
        
        # 交叉注意力（需要转换维度）
        h_t = h.transpose(1, 2)  # [batch, seq_len, out_dim]
        h_t = h_t + self.cross_attn(h_t, condition)
        h = h_t.transpose(1, 2)  # [batch, out_dim, seq_len]
        
        h = F.relu(h)
        
        return h + residual


class ConditionalUNet(nn.Module):
    """条件U-Net架构"""
    
    def __init__(
        self,
        in_channels: int = 4,  # DNA的4个碱基
        out_channels: int = 4,
        condition_dim: int = 64,
        base_dim: int = 64,
        dim_mults: Tuple[int, ...] = (1, 2, 4, 8),
        time_dim: int = 128
    ):
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.condition_dim = condition_dim
        self.time_dim = time_dim
        
        # 时间嵌入
        self.time_embed = nn.Sequential(
            nn.Linear(time_dim, time_dim * 2),
            nn.ReLU(),
            nn.Linear(time_dim * 2, time_dim)
        )
        
        # 条件嵌入
        self.condition_embed = ConditionalEmbedding(condition_dim)
        
        # 计算各层维度
        dims = [base_dim * m for m in dim_mults]
        in_out_dims = [(in_channels, dims[0])] + [(dims[i], dims[i+1]) for i in range(len(dims)-1)]
        
        # 下采样路径
        self.down_blocks = nn.ModuleList([])
        for ind, (dim_in, dim_out) in enumerate(in_out_dims):
            self.down_blocks.append(nn.ModuleList([
                ConditionalUNetBlock(dim_in, dim_out, condition_dim, time_dim),
                ConditionalUNetBlock(dim_out, dim_out, condition_dim, time_dim),
                nn.Conv1d(dim_out, dim_out, 4, 2, 1) if ind < len(in_out_dims) - 1 else nn.Identity()
            ]))
        
        # 中间层
        mid_dim = dims[-1]
        self.mid_block1 = ConditionalUNetBlock(mid_dim, mid_dim, condition_dim, time_dim)
        self.mid_block2 = ConditionalUNetBlock(mid_dim, mid_dim, condition_dim, time_dim)
        
        # 上采样路径
        self.up_blocks = nn.ModuleList([])
        for ind, (dim_out, dim_in) in enumerate(reversed(in_out_dims[1:])):
            self.up_blocks.append(nn.ModuleList([
                nn.ConvTranspose1d(dim_in, dim_out, 4, 2, 1),
                ConditionalUNetBlock(dim_out * 2, dim_out, condition_dim, time_dim),
                ConditionalUNetBlock(dim_out, dim_out, condition_dim, time_dim),
            ]))
        
        # 输出层
        self.out_conv = nn.Conv1d(base_dim, out_channels, 1)
        
    def positional_encoding(self, timestep: torch.Tensor) -> torch.Tensor:
        """位置编码用于时间步"""
        device = timestep.device
        half_dim = self.time_dim // 2
        embeddings = math.log(10000) / (half_dim - 1)
        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)
        embeddings = timestep[:, None] * embeddings[None, :]
        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)
        return embeddings
    
    def forward(
        self,
        x: torch.Tensor,
        timestep: torch.Tensor,
        conditions: Optional[Dict[str, float]] = None
    ) -> torch.Tensor:
        """
        x: [batch, channels, seq_len] - 输入序列
        timestep: [batch] - 时间步
        conditions: Dict[str, float] - 条件参数
        """
        # 时间和条件嵌入
        time_emb = self.time_embed(self.positional_encoding(timestep))
        condition_emb = self.condition_embed(conditions)
        
        # 下采样
        skip_connections = []
        h = x
        
        for block1, block2, downsample in self.down_blocks:
            h = block1(h, time_emb, condition_emb)
            h = block2(h, time_emb, condition_emb)
            skip_connections.append(h)
            h = downsample(h)
        
        # 中间层
        h = self.mid_block1(h, time_emb, condition_emb)
        h = self.mid_block2(h, time_emb, condition_emb)
        
        # 上采样
        for upsample, block1, block2 in self.up_blocks:
            h = upsample(h)
            h = torch.cat([h, skip_connections.pop()], dim=1)
            h = block1(h, time_emb, condition_emb)
            h = block2(h, time_emb, condition_emb)
        
        return self.out_conv(h)


class ConditionalDiffusionModel(nn.Module):
    """条件扩散模型主类"""
    
    def __init__(
        self,
        sequence_length: int = 1000,
        vocab_size: int = 4,
        condition_dim: int = 64,
        base_dim: int = 64,
        num_timesteps: int = 1000,
        beta_schedule: str = 'cosine'
    ):
        super().__init__()
        self.sequence_length = sequence_length
        self.vocab_size = vocab_size
        self.num_timesteps = num_timesteps
        
        # U-Net核心网络
        self.unet = ConditionalUNet(
            in_channels=vocab_size,
            out_channels=vocab_size,
            condition_dim=condition_dim,
            base_dim=base_dim
        )
        
        # 扩散参数
        self.register_buffer('betas', self._get_betas(beta_schedule, num_timesteps))
        self.register_buffer('alphas', 1.0 - self.betas)
        self.register_buffer('alphas_cumprod', torch.cumprod(self.alphas, dim=0))
        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(self.alphas_cumprod))
        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1.0 - self.alphas_cumprod))
        
    def _get_betas(self, schedule: str, timesteps: int) -> torch.Tensor:
        """获取噪声计划"""
        if schedule == 'linear':
            return torch.linspace(1e-4, 0.02, timesteps)
        elif schedule == 'cosine':
            s = 0.008
            steps = timesteps + 1
            x = torch.linspace(0, timesteps, steps)
            alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2
            alphas_cumprod = alphas_cumprod / alphas_cumprod[0]
            betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])
            return torch.clip(betas, 0.0001, 0.9999)
        else:
            raise ValueError(f"未知的噪声计划: {schedule}")
    
    def q_sample(self, x_start: torch.Tensor, t: torch.Tensor, noise: Optional[torch.Tensor] = None) -> torch.Tensor:
        """前向扩散过程"""
        if noise is None:
            noise = torch.randn_like(x_start)
        
        sqrt_alphas_cumprod_t = self.sqrt_alphas_cumprod[t][:, None, None]
        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t][:, None, None]
        
        return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise
    
    def predict_noise(
        self,
        x_t: torch.Tensor,
        t: torch.Tensor,
        conditions: Optional[Dict[str, float]] = None
    ) -> torch.Tensor:
        """预测噪声"""
        return self.unet(x_t, t, conditions)
    
    def p_losses(
        self,
        x_start: torch.Tensor,
        t: torch.Tensor,
        conditions: Optional[Dict[str, float]] = None,
        noise: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """计算损失"""
        if noise is None:
            noise = torch.randn_like(x_start)
        
        x_noisy = self.q_sample(x_start, t, noise)
        predicted_noise = self.predict_noise(x_noisy, t, conditions)
        
        return F.mse_loss(predicted_noise, noise)
    
    @torch.no_grad()
    def p_sample(
        self,
        x: torch.Tensor,
        t: torch.Tensor,
        t_index: int,
        conditions: Optional[Dict[str, float]] = None
    ) -> torch.Tensor:
        """单步采样"""
        betas_t = self.betas[t][:, None, None]
        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t][:, None, None]
        sqrt_recip_alphas_t = torch.sqrt(1.0 / self.alphas[t])[:, None, None]
        
        # 预测噪声
        predicted_noise = self.predict_noise(x, t, conditions)
        
        # 计算均值
        model_mean = sqrt_recip_alphas_t * (x - betas_t * predicted_noise / sqrt_one_minus_alphas_cumprod_t)
        
        if t_index == 0:
            return model_mean
        else:
            posterior_variance = betas_t
            noise = torch.randn_like(x)
            return model_mean + torch.sqrt(posterior_variance) * noise
    
    @torch.no_grad()
    def p_sample_loop(
        self,
        shape: Tuple[int, ...],
        conditions: Optional[Dict[str, float]] = None,
        device: Optional[torch.device] = None
    ) -> torch.Tensor:
        """完整采样循环"""
        device = device or next(self.parameters()).device
        b = shape[0]
        img = torch.randn(shape, device=device)
        
        for i in reversed(range(0, self.num_timesteps)):
            t = torch.full((b,), i, device=device, dtype=torch.long)
            img = self.p_sample(img, t, i, conditions)
            
        return img
    
    @torch.no_grad()
    def sample(
        self,
        batch_size: int = 1,
        conditions: Optional[Dict[str, float]] = None,
        device: Optional[torch.device] = None
    ) -> torch.Tensor:
        """生成样本"""
        return self.p_sample_loop(
            (batch_size, self.vocab_size, self.sequence_length),
            conditions,
            device
        )
    
    def forward(
        self,
        x: torch.Tensor,
        conditions: Optional[Dict[str, float]] = None
    ) -> torch.Tensor:
        """训练时的前向传播"""
        batch_size = x.shape[0]
        device = x.device
        
        # 随机采样时间步
        t = torch.randint(0, self.num_timesteps, (batch_size,), device=device).long()
        
        # 计算损失
        return self.p_losses(x, t, conditions)


def create_conditional_diffusion_model(
    sequence_length: int = 1000,
    vocab_size: int = 4,
    condition_dim: int = 64,
    **kwargs
) -> ConditionalDiffusionModel:
    """工厂函数：创建条件扩散模型"""
    return ConditionalDiffusionModel(
        sequence_length=sequence_length,
        vocab_size=vocab_size,
        condition_dim=condition_dim,
        **kwargs
    )


# 兼容性接口
class ConditionalDiffusionPredictor:
    """与现有预测器接口兼容的包装类"""
    
    def __init__(self, model: ConditionalDiffusionModel):
        self.model = model
        self.device = next(model.parameters()).device
        
    def predict(
        self,
        conditions: Optional[Dict[str, float]] = None,
        batch_size: int = 1
    ) -> torch.Tensor:
        """预测接口"""
        self.model.eval()
        with torch.no_grad():
            return self.model.sample(batch_size, conditions, self.device)
    
    def train_step(
        self,
        sequences: torch.Tensor,
        conditions: Optional[Dict[str, float]] = None
    ) -> float:
        """训练步骤"""
        self.model.train()
        loss = self.model(sequences, conditions)
        return loss.item()
    
    def to(self, device):
        """移动到设备"""
        self.model.to(device)
        self.device = device
        return self


if __name__ == "__main__":
    # 快速测试
    print("创建条件扩散模型...")
    model = create_conditional_diffusion_model()
    
    # 测试条件
    test_conditions = {
        'temperature': 42.0,
        'ph': 6.5,
        'oxygen': 0.15
    }
    
    print("测试采样...")
    samples = model.sample(batch_size=2, conditions=test_conditions)
    print(f"生成样本形状: {samples.shape}")
    
    # 测试训练
    print("测试训练...")
    x = torch.randn(2, 4, 1000)
    loss = model(x, test_conditions)
    print(f"训练损失: {loss.item():.4f}")
    
    print("条件扩散模型实现完成！")
