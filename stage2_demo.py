# -*- coding: utf-8 -*-
"""ç¬¬äºŒé˜¶æ®µæ¶æ„ä¼˜åŒ–æ¼”ç¤ºè„šæœ¬

å®Œæ•´æ¼”ç¤ºæ‰€æœ‰å››ä¸ªå…³é”®ç»„ä»¶çš„åŠŸèƒ½å’Œç”¨æ³•
"""

import torch
import numpy as np
import pandas as pd
from pathlib import Path
import json
import time
from typing import List, Dict, Any

# å¯¼å…¥æˆ‘ä»¬çš„ç»„ä»¶
from models.multimodal_fusion import create_multimodal_fusion_model, MultiModalPredictor
from training.advanced_trainer import create_advanced_trainer
from evaluation.biological_metrics import BiologicalMetrics, evaluate_generated_sequences
from one_click_pipeline import OneClickPipeline
from utils.logger import setup_logger

def demo_multimodal_fusion():
    """æ¼”ç¤ºå¤šæ¨¡æ€ç‰¹å¾èåˆåŠŸèƒ½"""
    print("\n" + "="*60)
    print("ğŸ§¬ æ¼”ç¤º A: å¤šæ¨¡æ€ç‰¹å¾èåˆä¼˜åŒ–")
    print("="*60)
    
    # åˆ›å»ºæ¨¡å‹é…ç½®
    config = {
        'vocab_size': 5,
        'seq_len': 200,  # å‡å°ä»¥åŠ å¿«æ¼”ç¤º
        'embed_dim': 128,
        'hidden_dim': 128,
        'output_dim': 64
    }
    
    print(f"ğŸ› ï¸ åˆ›å»ºå¤šæ¨¡æ€èåˆæ¨¡å‹...")
    print(f"é…ç½®: {config}")
    
    # åˆ›å»ºæ¨¡å‹
    model = create_multimodal_fusion_model(config)
    print(f"âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œå‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # æµ‹è¯•æ•°æ®
    batch_size = 4
    seq_len = config['seq_len']
    test_sequences = torch.randint(0, 5, (batch_size, seq_len))
    
    print(f"\nğŸ“Š æµ‹è¯•å‰å‘ä¼ æ’­...")
    print(f"è¾“å…¥å½¢çŠ¶: {test_sequences.shape}")
    
    # å‰å‘ä¼ æ’­æµ‹è¯•
    model.eval()
    with torch.no_grad():
        start_time = time.time()
        results = model(test_sequences)
        inference_time = time.time() - start_time
        
    print(f"âœ“ å‰å‘ä¼ æ’­æˆåŠŸï¼Œè€—æ—¶: {inference_time*1000:.2f}ms")
    print(f"é¢„æµ‹ç»“æœå½¢çŠ¶: {results['predictions'].shape}")
    print(f"èåˆç‰¹å¾å½¢çŠ¶: {results['fused_features'].shape}")
    
    if results['attention_weights'] is not None:
        print(f"æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {results['attention_weights'].shape}")
        print(f"æ³¨æ„åŠ›æƒé‡ç»Ÿè®¡: æœ€å¤§={results['attention_weights'].max():.3f}, æœ€å°={results['attention_weights'].min():.3f}")
        
    print(f"èåˆæƒé‡å½¢çŠ¶: {results['fusion_weights'].shape}")
    print(f"èåˆæƒé‡: {results['fusion_weights'][0].detach().numpy()}")
    
    print("âœ“ å¤šæ¨¡æ€ç‰¹å¾èåˆæ¼”ç¤ºå®Œæˆ")
    
def demo_advanced_training():
    """æ¼”ç¤ºé«˜çº§è®­ç»ƒåŠŸèƒ½"""
    print("\n" + "="*60)
    print("âš™ï¸ æ¼”ç¤º B: é«˜çº§è®­ç»ƒæµç¨‹")
    print("="*60)
    
    # åˆ›å»ºç®€å•æ¨¡å‹
    class SimpleModel(torch.nn.Module):
        def __init__(self):
            super().__init__()
            self.linear = torch.nn.Linear(50, 1)
            
        def forward(self, input_ids, **kwargs):
            return {'predictions': self.linear(input_ids.float())}
    
    model = SimpleModel()
    
    # è®­ç»ƒé…ç½®
    training_config = {
        'num_epochs': 5,  # å‡å°‘epochæ•°åŠ å¿«æ¼”ç¤º
        'use_amp': True,
        'max_grad_norm': 1.0,
        'early_stopping_patience': 10,
        'save_interval': 2,
        'eval_interval': 1,
        'log_interval': 10,
        'distributed': False,
        
        'optimizer': {
            'lr': 1e-3,
            'weight_decay': 1e-4,
            'betas': [0.9, 0.999]
        },
        
        'scheduler': {
            'type': 'cosine',
            'T_max': 5,
            'eta_min': 1e-6
        },
        
        'output_dir': 'demo_training_outputs'
    }
    
    print(f"ğŸ› ï¸ åˆ›å»ºé«˜çº§è®­ç»ƒå™¨...")
    print(f"ä¼˜åŒ–å™¨: AdamW, å­¦ä¹ ç‡è°ƒåº¦: Cosine")
    print(f"æ··åˆç²¾åº¦è®­ç»ƒ: {training_config['use_amp']}")
    print(f"æ¢¯åº¦è£å‰ª: {training_config['max_grad_norm']}")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger = setup_logger('demo_trainer', level='INFO')
    
    trainer = create_advanced_trainer(model, training_config, device, logger)
    print("âœ“ é«˜çº§è®­ç»ƒå™¨åˆ›å»ºæˆåŠŸ")
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    from torch.utils.data import TensorDataset, DataLoader
    
    print(f"\nğŸ“Š åˆ›å»ºæ¨¡æ‹Ÿè®­ç»ƒæ•°æ®...")
    
    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    X_train = torch.randn(200, 50)
    y_train = torch.randn(200, 1)
    X_val = torch.randn(50, 50)
    y_val = torch.randn(50, 1)
    
    def collate_fn(batch):
        inputs, targets = zip(*batch)
        return {
            'input_ids': torch.stack(inputs),
            'labels': torch.stack(targets)
        }
    
    train_dataset = TensorDataset(X_train, y_train)
    val_dataset = TensorDataset(X_val, y_val)
    
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)
    
    print(f"è®­ç»ƒæ•°æ®: {len(train_dataset)} æ ·æœ¬")
    print(f"éªŒè¯æ•°æ®: {len(val_dataset)} æ ·æœ¬")
    
    # è‡ªå®šä¹‰æŸå¤±å‡½æ•°
    def custom_criterion(outputs, batch):
        predictions = outputs['predictions']
        targets = batch['labels']
        return torch.nn.MSELoss()(predictions, targets)
    
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    start_time = time.time()
    
    # è®­ç»ƒæ¨¡å‹
    training_stats = trainer.train(
        train_loader=train_loader,
        val_loader=val_loader,
        criterion=custom_criterion
    )
    
    training_time = time.time() - start_time
    
    print(f"âœ“ è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {training_time:.2f}ç§’")
    print(f"æœ€ç»ˆè®­ç»ƒæŸå¤±: {training_stats['train_loss'][-1]:.6f}")
    if 'val_loss' in training_stats:
        print(f"æœ€ç»ˆéªŒè¯æŸå¤±: {training_stats['val_loss'][-1]:.6f}")
    
    print("âœ“ é«˜çº§è®­ç»ƒæµç¨‹æ¼”ç¤ºå®Œæˆ")
    
def demo_biological_evaluation():
    """æ¼”ç¤ºç”Ÿç‰©å­¦è¯„ä¼°åŠŸèƒ½"""
    print("\n" + "="*60)
    print("ğŸ§¬ æ¼”ç¤º C: ç”Ÿç‰©å­¦è¯„ä¼°ä½“ç³»")
    print("="*60)
    
    print("ğŸ› ï¸ åˆ›å»ºç”Ÿç‰©å­¦è¯„ä¼°å™¨...")
    evaluator = BiologicalMetrics()
    print("âœ“ ç”Ÿç‰©å­¦è¯„ä¼°å™¨åˆ›å»ºæˆåŠŸ")
    
    print(f"\nğŸ“Š ç”Ÿæˆæ¨¡æ‹Ÿåºåˆ—æ•°æ®...")
    
    # ç”Ÿæˆæ¨¡æ‹ŸçœŸå®åºåˆ—ï¼ˆGCå«é‡å¹³è¡¡ï¼‰
    np.random.seed(42)
    real_sequences = []
    for i in range(50):
        length = np.random.randint(100, 300)
        # ä¿æŒGCå«é‡åœ¨40-60%ä¹‹é—´
        seq = ''.join(np.random.choice(['A', 'T', 'G', 'C'], length, p=[0.3, 0.3, 0.2, 0.2]))
        real_sequences.append(seq)
    
    # ç”Ÿæˆæ¨¡æ‹Ÿç”Ÿæˆåºåˆ—ï¼ˆç¨æœ‰ä¸åŒçš„GCå«é‡ï¼‰
    generated_sequences = []
    for i in range(50):
        length = np.random.randint(100, 300)
        # GCå«é‡ç¨ä½
        seq = ''.join(np.random.choice(['A', 'T', 'G', 'C'], length, p=[0.35, 0.35, 0.15, 0.15]))
        generated_sequences.append(seq)
    
    print(f"çœŸå®åºåˆ—: {len(real_sequences)} æ¡")
    print(f"ç”Ÿæˆåºåˆ—: {len(generated_sequences)} æ¡")
    print(f"å¹³å‡åºåˆ—é•¿åº¦: çœŸå®={np.mean([len(s) for s in real_sequences]):.1f}, ç”Ÿæˆ={np.mean([len(s) for s in generated_sequences]):.1f}")
    
    # è¯„ä¼°æŒ‡æ ‡
    print(f"\nğŸ” è®¡ç®—ç”Ÿç‰©å­¦è¯„ä¼°æŒ‡æ ‡...")
    
    # 1. Jensen-Shannonæ•£åº¦
    print("è®¡ç®— Jensen-Shannon æ•£åº¦...")
    js_1mer = evaluator.jensen_shannon_divergence(real_sequences, generated_sequences, k=1)
    js_2mer = evaluator.jensen_shannon_divergence(real_sequences, generated_sequences, k=2)
    
    print(f"  JSæ•£åº¦ (1-mer): {js_1mer:.6f}")
    print(f"  JSæ•£åº¦ (2-mer): {js_2mer:.6f}")
    
    # 2. S-FID
    print("è®¡ç®— S-FID...")
    s_fid = evaluator.sequence_fid(real_sequences, generated_sequences)
    print(f"  S-FID: {s_fid:.6f}")
    
    # 3. GCå«é‡åˆ†æ
    print("åˆ†æ GCå«é‡...")
    real_gc = evaluator.gc_content(real_sequences)
    gen_gc = evaluator.gc_content(generated_sequences)
    
    print(f"  çœŸå®GCå«é‡: å¹³å‡={np.mean(real_gc):.3f}, æ ‡å‡†å·®={np.std(real_gc):.3f}")
    print(f"  ç”ŸæˆGCå«é‡: å¹³å‡={np.mean(gen_gc):.3f}, æ ‡å‡†å·®={np.std(gen_gc):.3f}")
    print(f"  GCå«é‡å·®å¼‚: {abs(np.mean(real_gc) - np.mean(gen_gc)):.6f}")
    
    # 4. Motifåˆ†æ
    print("åˆ†æ Motifs...")
    real_motifs = evaluator.count_motifs(real_sequences)
    gen_motifs = evaluator.count_motifs(generated_sequences)
    
    for motif_type in ['TATA_box', 'CpG_site']:
        real_count = np.mean(real_motifs[motif_type])
        gen_count = np.mean(gen_motifs[motif_type])
        print(f"  {motif_type}: çœŸå®={real_count:.2f}, ç”Ÿæˆ={gen_count:.2f}")
    
    # 5. åºåˆ—ç›¸ä¼¼åº¦
    print("è®¡ç®—åºåˆ—ç›¸ä¼¼åº¦...")
    similarity = evaluator.sequence_similarity(
        real_sequences[:20], generated_sequences[:20], method='kmer'
    )
    
    print(f"  ä½™å¼¦ç›¸ä¼¼åº¦: {similarity['cosine_similarity']:.6f}")
    print(f"  JSç›¸ä¼¼åº¦: {similarity['js_similarity']:.6f}")
    
    print("âœ“ ç”Ÿç‰©å­¦è¯„ä¼°æ¼”ç¤ºå®Œæˆ")
    
def demo_one_click_pipeline():
    """æ¼”ç¤ºä¸€é”®å¼æµç¨‹åŠŸèƒ½"""
    print("\n" + "="*60)
    print("ğŸš€ æ¼”ç¤º D: ä¸€é”®å¼è®­ç»ƒåˆ†ææµç¨‹")
    print("="*60)
    
    print("ğŸ› ï¸ å‡†å¤‡æ¨¡æ‹Ÿæ•°æ®...")
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    np.random.seed(42)
    sequences = []
    labels = []
    
    for i in range(100):  # å‡å°‘æ•°æ®é‡åŠ å¿«æ¼”ç¤º
        # ç”ŸæˆéšæœºDNAåºåˆ—
        length = np.random.randint(50, 200)  # å‡å°é•¿åº¦åŠ å¿«å¤„ç†
        seq = ''.join(np.random.choice(['A', 'T', 'G', 'C'], length))
        sequences.append(seq)
        
        # ç”Ÿæˆæ¨¡æ‹Ÿçš„å¯åŠ¨å­å¼ºåº¦æ ‡ç­¾
        gc_content = (seq.count('G') + seq.count('C')) / len(seq)
        strength = np.random.normal(0.5, 0.2) + 0.3 * (1 - abs(gc_content - 0.5) * 2)
        strength = np.clip(strength, 0, 1)
        labels.append(strength)
    
    print(f"æ•°æ®é‡: {len(sequences)} æ¡åºåˆ—")
    print(f"å¹³å‡åºåˆ—é•¿åº¦: {np.mean([len(seq) for seq in sequences]):.1f}")
    print(f"å¹³å‡å¼ºåº¦: {np.mean(labels):.3f}")
    
    # åˆ›å»ºç®€åŒ–é…ç½®
    config = {
        'data': {
            'max_length': 200,
            'batch_size': 8,
            'num_workers': 0
        },
        'model': {
            'embed_dim': 64,
            'hidden_dim': 64, 
            'output_dim': 32
        },
        'training': {
            'num_epochs': 3,
            'early_stopping_patience': 2,
            'eval_interval': 1,
            'optimizer': {
                'lr': 1e-3
            }
        },
        'hyperparameter_tuning': {
            'enabled': False
        },
        'output_dir': 'demo_pipeline_results'
    }
    
    print(f"\nğŸ› ï¸ åˆ›å»ºä¸€é”®å¼æµç¨‹...")
    pipeline = OneClickPipeline()
    pipeline.config = config  # ä½¿ç”¨æ¼”ç¤ºé…ç½®
    pipeline.output_dir = Path('demo_pipeline_results')
    pipeline.output_dir.mkdir(exist_ok=True)
    
    print("âœ“ æµç¨‹åˆå§‹åŒ–æˆåŠŸ")
    
    print(f"\nğŸš€ è¿è¡Œå®Œæ•´æµç¨‹...")
    start_time = time.time()
    
    try:
        results = pipeline.run_complete_pipeline(sequences, labels)
        pipeline_time = time.time() - start_time
        
        print(f"âœ“ ä¸€é”®å¼æµç¨‹æˆåŠŸå®Œæˆï¼Œè€—æ—¶: {pipeline_time:.2f}ç§’")
        
        if 'evaluation_results' in results and 'predictions' in results['evaluation_results']:
            pred_metrics = results['evaluation_results']['predictions']
            print(f"\nğŸ“ˆ æ¨¡å‹æ€§èƒ½:")
            print(f"  MSE: {pred_metrics['mse']:.6f}")
            print(f"  MAE: {pred_metrics['mae']:.6f}")
            print(f"  RÂ²: {pred_metrics['r2']:.6f}")
        
        print(f"\nğŸ“ ç»“æœæ–‡ä»¶:")
        output_dir = Path('demo_pipeline_results')
        if output_dir.exists():
            for file in output_dir.glob('*'):
                if file.is_file():
                    print(f"  - {file.name}")
        
    except Exception as e:
        print(f"âš ï¸ æµç¨‹æ‰§è¡Œå‡ºç°é—®é¢˜: {e}")
        print("è¿™åœ¨æ¼”ç¤ºç¯å¢ƒä¸­æ˜¯æ­£å¸¸çš„ï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦å®Œæ•´çš„ä¾èµ–ç¯å¢ƒ")
    
    print("âœ“ ä¸€é”®å¼æµç¨‹æ¼”ç¤ºå®Œæˆ")

def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("ğŸ† DNAå¯åŠ¨å­é¢„æµ‹ - ç¬¬äºŒé˜¶æ®µæ¶æ„ä¼˜åŒ–æ¼”ç¤º")
    print("ğŸ“ åŒ…å«å››ä¸ªå…³é”®ç»„ä»¶çš„å®Œæ•´æ¼”ç¤º")
    print("\nğŸ” ç»„ä»¶æ¦‚è§ˆ:")
    print("  A. å¤šæ¨¡æ€ç‰¹å¾èåˆä¼˜åŒ– - è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶")
    print("  B. é«˜çº§è®­ç»ƒæµç¨‹å®Œå–„ - åˆ†å¸ƒå¼è®­ç»ƒå’Œä¼˜åŒ–ç­–ç•¥")
    print("  C. ç”Ÿç‰©å­¦è¯„ä¼°ä½“ç³» - Jensen-Shannonæ•£åº¦å’ŒS-FID")
    print("  D. ä¸€é”®å¼è®­ç»ƒåˆ†ææµç¨‹ - å®Œæ•´è‡ªåŠ¨åŒ–è§£å†³æ–¹æ¡ˆ")
    
    try:
        # æ¼”ç¤º A: å¤šæ¨¡æ€ç‰¹å¾èåˆ
        demo_multimodal_fusion()
        
        # æ¼”ç¤º B: é«˜çº§è®­ç»ƒ
        demo_advanced_training()
        
        # æ¼”ç¤º C: ç”Ÿç‰©å­¦è¯„ä¼°
        demo_biological_evaluation()
        
        # æ¼”ç¤º D: ä¸€é”®å¼æµç¨‹
        demo_one_click_pipeline()
        
        print("\n" + "="*80)
        print("ğŸ‰ æ‰€æœ‰ç»„ä»¶æ¼”ç¤ºå®Œæˆï¼")
        print("="*80)
        
        print("\nğŸ“ˆ æ€§èƒ½æ€»ç»“:")
        print("âœ“ å¤šæ¨¡æ€èåˆ: å®ç°äº†è·¨æ¨¡æ€æ³¨æ„åŠ›å’Œè‡ªé€‚åº”æƒé‡èåˆ")
        print("âœ“ é«˜çº§è®­ç»ƒ: é›†æˆäº†æ··åˆç²¾åº¦ã€æ¢¯åº¦è£å‰ªå’Œå…ˆè¿›ä¼˜åŒ–ç­–ç•¥")
        print("âœ“ ç”Ÿç‰©å­¦è¯„ä¼°: æä¾›äº†JSæ•£åº¦ã€S-FIDã€Motifåˆ†æç­‰å…¨é¢æŒ‡æ ‡")
        print("âœ“ ä¸€é”®å¼æµç¨‹: å®ç°äº†ä»æ•°æ®åŠ è½½åˆ°ç»“æœåˆ†æçš„å®Œæ•´è‡ªåŠ¨åŒ–")
        
        print("\nğŸš€ ä¸‹ä¸€æ­¥:")
        print("1. åœ¨çœŸå®æ•°æ®ä¸Šæµ‹è¯•å„ä¸ªç»„ä»¶")
        print("2. æ ¹æ®å…·ä½“éœ€æ±‚è°ƒæ•´æ¨¡å‹æ¶æ„å’Œå‚æ•°")
        print("3. ä½¿ç”¨è¶…å‚æ•°è°ƒä¼˜åŠŸèƒ½ä¼˜åŒ–æ¨¡å‹æ€§èƒ½")
        print("4. éƒ¨ç½²åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­è¿›è¡Œå¤§è§„æ¨¡è®­ç»ƒ")
        
    except Exception as e:
        print(f"\nâš ï¸ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é—®é¢˜: {e}")
        print("è¯·æ£€æŸ¥ä¾èµ–ç¯å¢ƒå’Œæ¨¡å—å¯¼å…¥")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
